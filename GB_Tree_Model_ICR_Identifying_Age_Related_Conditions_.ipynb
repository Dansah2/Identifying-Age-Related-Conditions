{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dansah2/Identifying-Age-Related-Conditions/blob/main/GB_Tree_Model_ICR_Identifying_Age_Related_Conditions_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NWrKiQ4fRGS"
      },
      "source": [
        "# ICR - Identifying Age-Related Conditions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf6F6FaOB0dQ"
      },
      "source": [
        "Kaggle Dataset Download API Command:\n",
        "\n",
        "kaggle competitions download -c icr-identify-age-related-conditions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXlEc616CFtt"
      },
      "source": [
        "Predict whether a subject has or has not been diagnosed with one of these conditions -- a binary classification problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEHxmjhghVvK"
      },
      "source": [
        "##Project Outline:\n",
        "\n",
        "1) Download the dataset\n",
        "\n",
        "2) Explore/Analyze the data\n",
        "\n",
        "3) Preprocess and organize the data\n",
        "\n",
        "4) Create and Train baseline Model\n",
        "\n",
        "5) Save the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqdmA7kIfkiE"
      },
      "source": [
        "## Download the Dataset\n",
        "\n",
        "1) Install required libraries\n",
        "\n",
        "2) Import required libraries\n",
        "\n",
        "3) Obtain the preprocessed data previously saved to Google Drive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khuyKRt5FkpA"
      },
      "source": [
        "#### Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U keras-tuner\n",
        "!pip install -q -U scikit-learn\n",
        "!pip install -q -U numpy\n",
        "!pip install -q -U tensorflow_decision_forests\n",
        "!pip install -q -U bentoml"
      ],
      "metadata": {
        "id": "UtRLXXUXfqMV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "902c1e93-8888-415d-e575-9d2102f41974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.2 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.3 which is incompatible.\n",
            "pydantic 2.2.1 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.6.1 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.6/200.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.9/69.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ylR8aSxIbKf"
      },
      "source": [
        "#### Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading and handeling data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# model training\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_decision_forests as tfdf\n",
        "\n",
        "# downloading data\n",
        "from google.colab import drive\n",
        "\n",
        "# Training the model with k-fold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# hyperparameter tuning\n",
        "import keras_tuner as kt\n",
        "\n",
        "# reload model using bentoml\n",
        "from pathlib import Path\n",
        "import bentoml"
      ],
      "metadata": {
        "id": "QVtrLcfxftWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3N4YRGJIrmz"
      },
      "source": [
        "#### Obtain the preprocessed data previously saved to Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbeUxY8jQlaw",
        "outputId": "123a07e2-1dcb-455d-abe3-427d603cafbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive to store Kaggle API for future use\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5JLtC0lF8_I"
      },
      "outputs": [],
      "source": [
        "# create a function to read the data into a dataframe\n",
        "\n",
        "def read_function(csv_file):\n",
        "\n",
        "    return pd.read_csv(csv_file)\n",
        "\n",
        "clean_train = read_function('/content/drive/My Drive/ICR_Project/train_df.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create and Train baseline model\n",
        "1) Calculate the weights\n",
        "\n",
        "2) Create the model / callbacks\n",
        "\n",
        "3) Define the plot function\n",
        "\n",
        "4) Train the model"
      ],
      "metadata": {
        "id": "59kg4f6bjH4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Calculate the weights"
      ],
      "metadata": {
        "id": "d8duJCIok9o3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weights(train_df, target):\n",
        "  # Calculate the number of samples for each label.\n",
        "  neg, pos = np.bincount(train_df[target])\n",
        "\n",
        "  # Calculate total samples.\n",
        "  total = neg + pos\n",
        "\n",
        "  # Calculate the weight for each label.\n",
        "  weight_for_0 = (1 / neg) * (total / 2.0)\n",
        "  weight_for_1 = (1 / pos) * (total / 2.0)\n",
        "\n",
        "  class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "  print(f'Weight for class 0: {weight_for_0:.2f}')\n",
        "  print(f'Weight for class 1: {weight_for_1:.2f}')\n",
        "\n",
        "  return class_weight\n",
        "\n",
        "class_weight = get_weights(clean_train, 'Class')"
      ],
      "metadata": {
        "id": "tcFrsdGDjIye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aa1f9b5-88d6-4441-dabd-620a77306bba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight for class 0: 0.61\n",
            "Weight for class 1: 2.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Select the model"
      ],
      "metadata": {
        "id": "9q_qRRgMjKyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Look at the models to select from\n",
        "tfdf.keras.get_all_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcFKzlgAcVj7",
        "outputId": "ab2adecf-267d-4591-ae43-c84cd5e42685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensorflow_decision_forests.keras.RandomForestModel,\n",
              " tensorflow_decision_forests.keras.GradientBoostedTreesModel,\n",
              " tensorflow_decision_forests.keras.CartModel,\n",
              " tensorflow_decision_forests.keras.DistributedGradientBoostedTreesModel]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check config options\n",
        "model = tfdf.keras.GradientBoostedTreesModel(hyperparameter_template=\"benchmark_rank1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqgVsb3zcXoQ",
        "outputId": "25bf0a09-19d4-451e-8403-c06af56a806a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resolve hyper-parameter template \"benchmark_rank1\" to \"benchmark_rank1@v1\" -> {'growing_strategy': 'BEST_FIRST_GLOBAL', 'categorical_algorithm': 'RANDOM', 'split_axis': 'SPARSE_OBLIQUE', 'sparse_oblique_normalization': 'MIN_MAX', 'sparse_oblique_num_projections_exponent': 1.0}.\n",
            "Use /tmp/tmp4c1clar5 as temporary training directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tune hyperparameters\n"
      ],
      "metadata": {
        "id": "E8eqqT8ZfuJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_dataset = tfdf.keras.pd_dataframe_to_tf_dataset(clean_train, label=\"Class\")\n",
        "\n",
        "tuner = tfdf.tuner.RandomSearch(num_trials=20)\n",
        "\n",
        "# Hyper-parameters to optimize.\n",
        "tuner.choice(\"max_depth\", [3, 4, 5, 6, 7, 8, 9, 10])\n",
        "tuner.choice(\"num_trees\", [400, 500, 600, 700, 800])\n",
        "\n",
        "model = tfdf.keras.GradientBoostedTreesModel(tuner=tuner)\n",
        "model.fit(tf_dataset)\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gi6ClFBYub0B",
        "outputId": "e3407642-ea09-47ae-cf68-cde820f3a6ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use /tmp/tmpoyp0cyck as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:06.875294. Found 617 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:08.770881\n",
            "Compiling model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7d6234bb6050> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7d6234bb6050> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "Model compiled.\n",
            "Model: \"gradient_boosted_trees_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            "=================================================================\n",
            "Total params: 1 (1.00 Byte)\n",
            "Trainable params: 0 (0.00 Byte)\n",
            "Non-trainable params: 1 (1.00 Byte)\n",
            "_________________________________________________________________\n",
            "Type: \"GRADIENT_BOOSTED_TREES\"\n",
            "Task: CLASSIFICATION\n",
            "Label: \"__LABEL\"\n",
            "\n",
            "Input Features (55):\n",
            "\tAB\n",
            "\tAF\n",
            "\tAH\n",
            "\tAM\n",
            "\tAR\n",
            "\tAX\n",
            "\tAY\n",
            "\tAZ\n",
            "\tBC\n",
            "\tBD_\n",
            "\tBN\n",
            "\tBP\n",
            "\tBQ\n",
            "\tBR\n",
            "\tBZ\n",
            "\tCB\n",
            "\tCC\n",
            "\tCD_\n",
            "\tCF\n",
            "\tCH\n",
            "\tCL\n",
            "\tCR\n",
            "\tCS\n",
            "\tCU\n",
            "\tCW_\n",
            "\tDA\n",
            "\tDE\n",
            "\tDF\n",
            "\tDH\n",
            "\tDI\n",
            "\tDL\n",
            "\tDN\n",
            "\tDU\n",
            "\tDV\n",
            "\tDY\n",
            "\tEB\n",
            "\tEE\n",
            "\tEG\n",
            "\tEH\n",
            "\tEL\n",
            "\tEP\n",
            "\tEU\n",
            "\tFC\n",
            "\tFD_\n",
            "\tFE\n",
            "\tFI\n",
            "\tFL\n",
            "\tFR\n",
            "\tFS\n",
            "\tGB\n",
            "\tGE\n",
            "\tGF\n",
            "\tGH\n",
            "\tGI\n",
            "\tGL\n",
            "\n",
            "No weights\n",
            "\n",
            "Variable Importance: INV_MEAN_MIN_DEPTH:\n",
            "    1.  \"DU\"  0.398977 ################\n",
            "    2.  \"CC\"  0.364146 ######\n",
            "    3.  \"BQ\"  0.360777 #####\n",
            "    4.  \"GL\"  0.358786 #####\n",
            "    5.  \"FR\"  0.356002 ####\n",
            "    6.  \"AB\"  0.349776 ###\n",
            "    7.  \"CH\"  0.349306 ##\n",
            "    8.  \"EH\"  0.349150 ##\n",
            "    9.  \"EL\"  0.348370 ##\n",
            "   10.  \"CR\"  0.347904 ##\n",
            "   11.  \"CS\"  0.347439 ##\n",
            "   12.  \"DL\"  0.347439 ##\n",
            "   13.  \"FE\"  0.346975 ##\n",
            "   14.  \"DA\"  0.346667 ##\n",
            "   15.  \"EU\"  0.346205 ##\n",
            "   16.  \"DE\"  0.345591 #\n",
            "   17.  \"CU\"  0.344980 #\n",
            "   18. \"CD_\"  0.344828 #\n",
            "   19.  \"EB\"  0.344675 #\n",
            "   20.  \"BR\"  0.344371 #\n",
            "   21.  \"AF\"  0.344219 #\n",
            "   22.  \"DH\"  0.343764 #\n",
            "   23.  \"FL\"  0.343310 #\n",
            "   24.  \"BC\"  0.342556 #\n",
            "   25.  \"EE\"  0.342406 #\n",
            "   26.  \"AH\"  0.341955 \n",
            "   27.  \"AX\"  0.341955 \n",
            "   28.  \"AY\"  0.341506 \n",
            "   29.  \"EG\"  0.341506 \n",
            "   30.  \"DI\"  0.340611 \n",
            "   31.  \"DY\"  0.340611 \n",
            "   32.  \"DN\"  0.340166 \n",
            "   33.  \"FI\"  0.340166 \n",
            "   34.  \"CL\"  0.339721 \n",
            "   35.  \"GH\"  0.339721 \n",
            "   36.  \"AM\"  0.339278 \n",
            "   37.  \"DF\"  0.339278 \n",
            "   38.  \"DV\"  0.339278 \n",
            "   39.  \"EP\"  0.339278 \n",
            "   40. \"FD_\"  0.339278 \n",
            "   41.  \"BP\"  0.338836 \n",
            "   42.  \"CB\"  0.338836 \n",
            "   43.  \"FS\"  0.338836 \n",
            "   44.  \"AR\"  0.338395 \n",
            "   45.  \"BN\"  0.338395 \n",
            "   46.  \"CF\"  0.338395 \n",
            "   47.  \"FC\"  0.338395 \n",
            "   48.  \"GB\"  0.338395 \n",
            "   49.  \"GE\"  0.338395 \n",
            "   50.  \"GI\"  0.338395 \n",
            "\n",
            "Variable Importance: NUM_AS_ROOT:\n",
            "    1.  \"DU\" 26.000000 ################\n",
            "    2.  \"CC\" 14.000000 ########\n",
            "    3.  \"BQ\" 11.000000 ######\n",
            "    4.  \"GL\" 11.000000 ######\n",
            "    5.  \"FR\"  7.000000 ###\n",
            "    6.  \"CH\"  6.000000 ###\n",
            "    7.  \"CS\"  5.000000 ##\n",
            "    8.  \"EL\"  5.000000 ##\n",
            "    9.  \"DA\"  4.000000 #\n",
            "   10.  \"DL\"  4.000000 #\n",
            "   11.  \"EH\"  4.000000 #\n",
            "   12.  \"EU\"  4.000000 #\n",
            "   13.  \"FE\"  4.000000 #\n",
            "   14.  \"AF\"  3.000000 #\n",
            "   15. \"CD_\"  3.000000 #\n",
            "   16.  \"CR\"  3.000000 #\n",
            "   17.  \"EB\"  3.000000 #\n",
            "   18.  \"AY\"  2.000000 \n",
            "   19.  \"BR\"  2.000000 \n",
            "   20.  \"CU\"  2.000000 \n",
            "   21.  \"DH\"  2.000000 \n",
            "   22.  \"AH\"  1.000000 \n",
            "   23.  \"BC\"  1.000000 \n",
            "   24.  \"DE\"  1.000000 \n",
            "   25.  \"DI\"  1.000000 \n",
            "   26.  \"FL\"  1.000000 \n",
            "\n",
            "Variable Importance: NUM_NODES:\n",
            "    1.  \"DU\" 39.000000 ################\n",
            "    2.  \"AB\" 26.000000 ##########\n",
            "    3.  \"FR\" 18.000000 #######\n",
            "    4.  \"CC\" 17.000000 ######\n",
            "    5.  \"EH\" 16.000000 ######\n",
            "    6.  \"BQ\" 15.000000 #####\n",
            "    7.  \"DE\" 14.000000 #####\n",
            "    8.  \"GL\" 14.000000 #####\n",
            "    9.  \"CR\" 13.000000 #####\n",
            "   10.  \"CU\" 11.000000 ####\n",
            "   11.  \"EE\" 10.000000 ###\n",
            "   12.  \"AX\"  9.000000 ###\n",
            "   13.  \"DL\"  9.000000 ###\n",
            "   14.  \"EL\"  9.000000 ###\n",
            "   15.  \"FL\"  9.000000 ###\n",
            "   16.  \"BC\"  8.000000 ##\n",
            "   17.  \"EG\"  8.000000 ##\n",
            "   18.  \"FE\"  8.000000 ##\n",
            "   19.  \"BR\"  7.000000 ##\n",
            "   20.  \"CH\"  7.000000 ##\n",
            "   21.  \"DA\"  7.000000 ##\n",
            "   22.  \"DH\"  7.000000 ##\n",
            "   23.  \"EU\"  7.000000 ##\n",
            "   24.  \"AH\"  6.000000 ##\n",
            "   25. \"CD_\"  6.000000 ##\n",
            "   26.  \"CS\"  6.000000 ##\n",
            "   27.  \"DY\"  6.000000 ##\n",
            "   28.  \"EB\"  6.000000 ##\n",
            "   29.  \"AF\"  5.000000 #\n",
            "   30.  \"DN\"  5.000000 #\n",
            "   31.  \"FI\"  5.000000 #\n",
            "   32.  \"CL\"  4.000000 #\n",
            "   33.  \"GH\"  4.000000 #\n",
            "   34.  \"AM\"  3.000000 \n",
            "   35.  \"DF\"  3.000000 \n",
            "   36.  \"DI\"  3.000000 \n",
            "   37.  \"DV\"  3.000000 \n",
            "   38.  \"EP\"  3.000000 \n",
            "   39. \"FD_\"  3.000000 \n",
            "   40.  \"AY\"  2.000000 \n",
            "   41.  \"BP\"  2.000000 \n",
            "   42.  \"CB\"  2.000000 \n",
            "   43.  \"FS\"  2.000000 \n",
            "   44.  \"AR\"  1.000000 \n",
            "   45.  \"BN\"  1.000000 \n",
            "   46.  \"CF\"  1.000000 \n",
            "   47.  \"FC\"  1.000000 \n",
            "   48.  \"GB\"  1.000000 \n",
            "   49.  \"GE\"  1.000000 \n",
            "   50.  \"GI\"  1.000000 \n",
            "\n",
            "Variable Importance: SUM_SCORE:\n",
            "    1.  \"DU\" 103.902576 ################\n",
            "    2.  \"CR\" 35.177587 #####\n",
            "    3.  \"DA\" 23.319519 ###\n",
            "    4.  \"BC\" 20.128741 ###\n",
            "    5.  \"AB\" 20.077489 ###\n",
            "    6.  \"FR\" 18.691368 ##\n",
            "    7.  \"BQ\" 16.551942 ##\n",
            "    8.  \"GL\" 13.142975 ##\n",
            "    9.  \"EG\" 12.671529 #\n",
            "   10.  \"DE\" 11.542636 #\n",
            "   11.  \"DY\"  9.362400 #\n",
            "   12.  \"CC\"  8.530472 #\n",
            "   13.  \"FE\"  6.378534 \n",
            "   14.  \"DH\"  5.480423 \n",
            "   15.  \"DI\"  5.373887 \n",
            "   16.  \"EE\"  4.813573 \n",
            "   17.  \"FI\"  4.257493 \n",
            "   18.  \"DL\"  4.234440 \n",
            "   19.  \"EL\"  3.974721 \n",
            "   20.  \"CU\"  3.885214 \n",
            "   21.  \"BR\"  3.441306 \n",
            "   22.  \"AF\"  3.411499 \n",
            "   23.  \"EH\"  3.180060 \n",
            "   24.  \"AX\"  3.167292 \n",
            "   25.  \"FL\"  2.175652 \n",
            "   26.  \"EP\"  2.049662 \n",
            "   27.  \"BP\"  1.597715 \n",
            "   28.  \"EB\"  1.450828 \n",
            "   29.  \"CS\"  1.145624 \n",
            "   30.  \"CH\"  1.059090 \n",
            "   31.  \"EU\"  1.013129 \n",
            "   32.  \"AR\"  0.921151 \n",
            "   33.  \"GH\"  0.900963 \n",
            "   34.  \"DN\"  0.887159 \n",
            "   35.  \"AM\"  0.824771 \n",
            "   36.  \"AH\"  0.785410 \n",
            "   37.  \"DV\"  0.757409 \n",
            "   38.  \"CL\"  0.689592 \n",
            "   39.  \"DF\"  0.663315 \n",
            "   40.  \"CB\"  0.654942 \n",
            "   41. \"CD_\"  0.649877 \n",
            "   42.  \"FS\"  0.642484 \n",
            "   43.  \"GE\"  0.341816 \n",
            "   44.  \"AY\"  0.335883 \n",
            "   45. \"FD_\"  0.332205 \n",
            "   46.  \"CF\"  0.222348 \n",
            "   47.  \"BN\"  0.161344 \n",
            "   48.  \"FC\"  0.118714 \n",
            "   49.  \"GB\"  0.072381 \n",
            "   50.  \"GI\"  0.069095 \n",
            "\n",
            "\n",
            "Hyperparameter optimizer:\n",
            "\n",
            "Best parameters: max_depth:3 num_trees:700\n",
            "Num steps: 20\n",
            "Best score: -0.464162\n",
            "\n",
            "Step #0 score:-0.573375 parameters:{ max_depth:4 num_trees:800 }\n",
            "Step #1 score:-0.681283 parameters:{ max_depth:10 num_trees:500 }\n",
            "Step #2 score:-0.577092 parameters:{ max_depth:5 num_trees:600 }\n",
            "Step #3 score:-0.681283 parameters:{ max_depth:10 num_trees:800 }\n",
            "Step #4 score:-0.613854 parameters:{ max_depth:8 num_trees:800 }\n",
            "Step #5 score:-0.464162 parameters:{ max_depth:3 num_trees:700 }\n",
            "Step #6 score:-0.577092 parameters:{ max_depth:5 num_trees:400 }\n",
            "Step #7 score:-0.583949 parameters:{ max_depth:9 num_trees:500 }\n",
            "Step #8 score:-0.681283 parameters:{ max_depth:10 num_trees:600 }\n",
            "Step #9 score:-0.602719 parameters:{ max_depth:6 num_trees:700 }\n",
            "Step #10 score:-0.573375 parameters:{ max_depth:4 num_trees:700 }\n",
            "Step #11 score:-0.602719 parameters:{ max_depth:6 num_trees:600 }\n",
            "Step #12 score:-0.573375 parameters:{ max_depth:4 num_trees:500 }\n",
            "Step #13 score:-0.583949 parameters:{ max_depth:9 num_trees:400 }\n",
            "Step #14 score:-0.464162 parameters:{ max_depth:3 num_trees:400 }\n",
            "Step #15 score:-0.613854 parameters:{ max_depth:8 num_trees:400 }\n",
            "Step #16 score:-0.613854 parameters:{ max_depth:8 num_trees:700 }\n",
            "Step #17 score:-0.638157 parameters:{ max_depth:7 num_trees:800 }\n",
            "Step #18 score:-0.583949 parameters:{ max_depth:9 num_trees:800 }\n",
            "Step #19 score:-0.638157 parameters:{ max_depth:7 num_trees:700 }\n",
            "\n",
            "\n",
            "Loss: BINOMIAL_LOG_LIKELIHOOD\n",
            "Validation loss value: 0.464162\n",
            "Number of trees per iteration: 1\n",
            "Node format: NOT_SET\n",
            "Number of trees: 130\n",
            "Total number of nodes: 878\n",
            "\n",
            "Number of nodes by tree:\n",
            "Count: 130 Average: 6.75385 StdDev: 0.657051\n",
            "Min: 5 Max: 7 Ignored: 0\n",
            "----------------------------------------------\n",
            "[ 5, 6)  16  12.31%  12.31% #\n",
            "[ 6, 7)   0   0.00%  12.31%\n",
            "[ 7, 7] 114  87.69% 100.00% ##########\n",
            "\n",
            "Depth by leafs:\n",
            "Count: 504 Average: 1.96825 StdDev: 0.175323\n",
            "Min: 1 Max: 2 Ignored: 0\n",
            "----------------------------------------------\n",
            "[ 1, 2)  16   3.17%   3.17%\n",
            "[ 2, 2] 488  96.83% 100.00% ##########\n",
            "\n",
            "Number of training obs by leaf:\n",
            "Count: 504 Average: 144.444 StdDev: 163.629\n",
            "Min: 5 Max: 545 Ignored: 0\n",
            "----------------------------------------------\n",
            "[   5,  32) 185  36.71%  36.71% ##########\n",
            "[  32,  59)  68  13.49%  50.20% ####\n",
            "[  59,  86)  35   6.94%  57.14% ##\n",
            "[  86, 113)  27   5.36%  62.50% #\n",
            "[ 113, 140)  11   2.18%  64.68% #\n",
            "[ 140, 167)  23   4.56%  69.25% #\n",
            "[ 167, 194)  13   2.58%  71.83% #\n",
            "[ 194, 221)   8   1.59%  73.41%\n",
            "[ 221, 248)  11   2.18%  75.60% #\n",
            "[ 248, 275)   6   1.19%  76.79%\n",
            "[ 275, 302)   3   0.60%  77.38%\n",
            "[ 302, 329)   8   1.59%  78.97%\n",
            "[ 329, 356)  13   2.58%  81.55% #\n",
            "[ 356, 383)  15   2.98%  84.52% #\n",
            "[ 383, 410)  18   3.57%  88.10% #\n",
            "[ 410, 437)  18   3.57%  91.67% #\n",
            "[ 437, 464)   8   1.59%  93.25%\n",
            "[ 464, 491)  16   3.17%  96.43% #\n",
            "[ 491, 518)   9   1.79%  98.21%\n",
            "[ 518, 545]   9   1.79% 100.00%\n",
            "\n",
            "Attribute in nodes:\n",
            "\t39 : DU [NUMERICAL]\n",
            "\t26 : AB [NUMERICAL]\n",
            "\t18 : FR [NUMERICAL]\n",
            "\t17 : CC [NUMERICAL]\n",
            "\t16 : EH [NUMERICAL]\n",
            "\t15 : BQ [NUMERICAL]\n",
            "\t14 : GL [NUMERICAL]\n",
            "\t14 : DE [NUMERICAL]\n",
            "\t13 : CR [NUMERICAL]\n",
            "\t11 : CU [NUMERICAL]\n",
            "\t10 : EE [NUMERICAL]\n",
            "\t9 : FL [NUMERICAL]\n",
            "\t9 : EL [NUMERICAL]\n",
            "\t9 : DL [NUMERICAL]\n",
            "\t9 : AX [NUMERICAL]\n",
            "\t8 : FE [NUMERICAL]\n",
            "\t8 : EG [NUMERICAL]\n",
            "\t8 : BC [NUMERICAL]\n",
            "\t7 : EU [NUMERICAL]\n",
            "\t7 : DH [NUMERICAL]\n",
            "\t7 : DA [NUMERICAL]\n",
            "\t7 : CH [NUMERICAL]\n",
            "\t7 : BR [NUMERICAL]\n",
            "\t6 : EB [NUMERICAL]\n",
            "\t6 : DY [NUMERICAL]\n",
            "\t6 : CS [NUMERICAL]\n",
            "\t6 : CD_ [NUMERICAL]\n",
            "\t6 : AH [NUMERICAL]\n",
            "\t5 : FI [NUMERICAL]\n",
            "\t5 : DN [NUMERICAL]\n",
            "\t5 : AF [NUMERICAL]\n",
            "\t4 : GH [NUMERICAL]\n",
            "\t4 : CL [NUMERICAL]\n",
            "\t3 : FD_ [NUMERICAL]\n",
            "\t3 : EP [NUMERICAL]\n",
            "\t3 : DV [NUMERICAL]\n",
            "\t3 : DI [NUMERICAL]\n",
            "\t3 : DF [NUMERICAL]\n",
            "\t3 : AM [NUMERICAL]\n",
            "\t2 : FS [NUMERICAL]\n",
            "\t2 : CB [NUMERICAL]\n",
            "\t2 : BP [NUMERICAL]\n",
            "\t2 : AY [NUMERICAL]\n",
            "\t1 : GI [NUMERICAL]\n",
            "\t1 : GE [NUMERICAL]\n",
            "\t1 : GB [NUMERICAL]\n",
            "\t1 : FC [NUMERICAL]\n",
            "\t1 : CF [NUMERICAL]\n",
            "\t1 : BN [NUMERICAL]\n",
            "\t1 : AR [NUMERICAL]\n",
            "\n",
            "Attribute in nodes with depth <= 0:\n",
            "\t26 : DU [NUMERICAL]\n",
            "\t14 : CC [NUMERICAL]\n",
            "\t11 : GL [NUMERICAL]\n",
            "\t11 : BQ [NUMERICAL]\n",
            "\t7 : FR [NUMERICAL]\n",
            "\t6 : CH [NUMERICAL]\n",
            "\t5 : EL [NUMERICAL]\n",
            "\t5 : CS [NUMERICAL]\n",
            "\t4 : FE [NUMERICAL]\n",
            "\t4 : EU [NUMERICAL]\n",
            "\t4 : EH [NUMERICAL]\n",
            "\t4 : DL [NUMERICAL]\n",
            "\t4 : DA [NUMERICAL]\n",
            "\t3 : EB [NUMERICAL]\n",
            "\t3 : CR [NUMERICAL]\n",
            "\t3 : CD_ [NUMERICAL]\n",
            "\t3 : AF [NUMERICAL]\n",
            "\t2 : DH [NUMERICAL]\n",
            "\t2 : CU [NUMERICAL]\n",
            "\t2 : BR [NUMERICAL]\n",
            "\t2 : AY [NUMERICAL]\n",
            "\t1 : FL [NUMERICAL]\n",
            "\t1 : DI [NUMERICAL]\n",
            "\t1 : DE [NUMERICAL]\n",
            "\t1 : BC [NUMERICAL]\n",
            "\t1 : AH [NUMERICAL]\n",
            "\n",
            "Attribute in nodes with depth <= 1:\n",
            "\t39 : DU [NUMERICAL]\n",
            "\t26 : AB [NUMERICAL]\n",
            "\t18 : FR [NUMERICAL]\n",
            "\t17 : CC [NUMERICAL]\n",
            "\t16 : EH [NUMERICAL]\n",
            "\t15 : BQ [NUMERICAL]\n",
            "\t14 : GL [NUMERICAL]\n",
            "\t14 : DE [NUMERICAL]\n",
            "\t13 : CR [NUMERICAL]\n",
            "\t11 : CU [NUMERICAL]\n",
            "\t10 : EE [NUMERICAL]\n",
            "\t9 : FL [NUMERICAL]\n",
            "\t9 : EL [NUMERICAL]\n",
            "\t9 : DL [NUMERICAL]\n",
            "\t9 : AX [NUMERICAL]\n",
            "\t8 : FE [NUMERICAL]\n",
            "\t8 : EG [NUMERICAL]\n",
            "\t8 : BC [NUMERICAL]\n",
            "\t7 : EU [NUMERICAL]\n",
            "\t7 : DH [NUMERICAL]\n",
            "\t7 : DA [NUMERICAL]\n",
            "\t7 : CH [NUMERICAL]\n",
            "\t7 : BR [NUMERICAL]\n",
            "\t6 : EB [NUMERICAL]\n",
            "\t6 : DY [NUMERICAL]\n",
            "\t6 : CS [NUMERICAL]\n",
            "\t6 : CD_ [NUMERICAL]\n",
            "\t6 : AH [NUMERICAL]\n",
            "\t5 : FI [NUMERICAL]\n",
            "\t5 : DN [NUMERICAL]\n",
            "\t5 : AF [NUMERICAL]\n",
            "\t4 : GH [NUMERICAL]\n",
            "\t4 : CL [NUMERICAL]\n",
            "\t3 : FD_ [NUMERICAL]\n",
            "\t3 : EP [NUMERICAL]\n",
            "\t3 : DV [NUMERICAL]\n",
            "\t3 : DI [NUMERICAL]\n",
            "\t3 : DF [NUMERICAL]\n",
            "\t3 : AM [NUMERICAL]\n",
            "\t2 : FS [NUMERICAL]\n",
            "\t2 : CB [NUMERICAL]\n",
            "\t2 : BP [NUMERICAL]\n",
            "\t2 : AY [NUMERICAL]\n",
            "\t1 : GI [NUMERICAL]\n",
            "\t1 : GE [NUMERICAL]\n",
            "\t1 : GB [NUMERICAL]\n",
            "\t1 : FC [NUMERICAL]\n",
            "\t1 : CF [NUMERICAL]\n",
            "\t1 : BN [NUMERICAL]\n",
            "\t1 : AR [NUMERICAL]\n",
            "\n",
            "Attribute in nodes with depth <= 2:\n",
            "\t39 : DU [NUMERICAL]\n",
            "\t26 : AB [NUMERICAL]\n",
            "\t18 : FR [NUMERICAL]\n",
            "\t17 : CC [NUMERICAL]\n",
            "\t16 : EH [NUMERICAL]\n",
            "\t15 : BQ [NUMERICAL]\n",
            "\t14 : GL [NUMERICAL]\n",
            "\t14 : DE [NUMERICAL]\n",
            "\t13 : CR [NUMERICAL]\n",
            "\t11 : CU [NUMERICAL]\n",
            "\t10 : EE [NUMERICAL]\n",
            "\t9 : FL [NUMERICAL]\n",
            "\t9 : EL [NUMERICAL]\n",
            "\t9 : DL [NUMERICAL]\n",
            "\t9 : AX [NUMERICAL]\n",
            "\t8 : FE [NUMERICAL]\n",
            "\t8 : EG [NUMERICAL]\n",
            "\t8 : BC [NUMERICAL]\n",
            "\t7 : EU [NUMERICAL]\n",
            "\t7 : DH [NUMERICAL]\n",
            "\t7 : DA [NUMERICAL]\n",
            "\t7 : CH [NUMERICAL]\n",
            "\t7 : BR [NUMERICAL]\n",
            "\t6 : EB [NUMERICAL]\n",
            "\t6 : DY [NUMERICAL]\n",
            "\t6 : CS [NUMERICAL]\n",
            "\t6 : CD_ [NUMERICAL]\n",
            "\t6 : AH [NUMERICAL]\n",
            "\t5 : FI [NUMERICAL]\n",
            "\t5 : DN [NUMERICAL]\n",
            "\t5 : AF [NUMERICAL]\n",
            "\t4 : GH [NUMERICAL]\n",
            "\t4 : CL [NUMERICAL]\n",
            "\t3 : FD_ [NUMERICAL]\n",
            "\t3 : EP [NUMERICAL]\n",
            "\t3 : DV [NUMERICAL]\n",
            "\t3 : DI [NUMERICAL]\n",
            "\t3 : DF [NUMERICAL]\n",
            "\t3 : AM [NUMERICAL]\n",
            "\t2 : FS [NUMERICAL]\n",
            "\t2 : CB [NUMERICAL]\n",
            "\t2 : BP [NUMERICAL]\n",
            "\t2 : AY [NUMERICAL]\n",
            "\t1 : GI [NUMERICAL]\n",
            "\t1 : GE [NUMERICAL]\n",
            "\t1 : GB [NUMERICAL]\n",
            "\t1 : FC [NUMERICAL]\n",
            "\t1 : CF [NUMERICAL]\n",
            "\t1 : BN [NUMERICAL]\n",
            "\t1 : AR [NUMERICAL]\n",
            "\n",
            "Attribute in nodes with depth <= 3:\n",
            "\t39 : DU [NUMERICAL]\n",
            "\t26 : AB [NUMERICAL]\n",
            "\t18 : FR [NUMERICAL]\n",
            "\t17 : CC [NUMERICAL]\n",
            "\t16 : EH [NUMERICAL]\n",
            "\t15 : BQ [NUMERICAL]\n",
            "\t14 : GL [NUMERICAL]\n",
            "\t14 : DE [NUMERICAL]\n",
            "\t13 : CR [NUMERICAL]\n",
            "\t11 : CU [NUMERICAL]\n",
            "\t10 : EE [NUMERICAL]\n",
            "\t9 : FL [NUMERICAL]\n",
            "\t9 : EL [NUMERICAL]\n",
            "\t9 : DL [NUMERICAL]\n",
            "\t9 : AX [NUMERICAL]\n",
            "\t8 : FE [NUMERICAL]\n",
            "\t8 : EG [NUMERICAL]\n",
            "\t8 : BC [NUMERICAL]\n",
            "\t7 : EU [NUMERICAL]\n",
            "\t7 : DH [NUMERICAL]\n",
            "\t7 : DA [NUMERICAL]\n",
            "\t7 : CH [NUMERICAL]\n",
            "\t7 : BR [NUMERICAL]\n",
            "\t6 : EB [NUMERICAL]\n",
            "\t6 : DY [NUMERICAL]\n",
            "\t6 : CS [NUMERICAL]\n",
            "\t6 : CD_ [NUMERICAL]\n",
            "\t6 : AH [NUMERICAL]\n",
            "\t5 : FI [NUMERICAL]\n",
            "\t5 : DN [NUMERICAL]\n",
            "\t5 : AF [NUMERICAL]\n",
            "\t4 : GH [NUMERICAL]\n",
            "\t4 : CL [NUMERICAL]\n",
            "\t3 : FD_ [NUMERICAL]\n",
            "\t3 : EP [NUMERICAL]\n",
            "\t3 : DV [NUMERICAL]\n",
            "\t3 : DI [NUMERICAL]\n",
            "\t3 : DF [NUMERICAL]\n",
            "\t3 : AM [NUMERICAL]\n",
            "\t2 : FS [NUMERICAL]\n",
            "\t2 : CB [NUMERICAL]\n",
            "\t2 : BP [NUMERICAL]\n",
            "\t2 : AY [NUMERICAL]\n",
            "\t1 : GI [NUMERICAL]\n",
            "\t1 : GE [NUMERICAL]\n",
            "\t1 : GB [NUMERICAL]\n",
            "\t1 : FC [NUMERICAL]\n",
            "\t1 : CF [NUMERICAL]\n",
            "\t1 : BN [NUMERICAL]\n",
            "\t1 : AR [NUMERICAL]\n",
            "\n",
            "Attribute in nodes with depth <= 5:\n",
            "\t39 : DU [NUMERICAL]\n",
            "\t26 : AB [NUMERICAL]\n",
            "\t18 : FR [NUMERICAL]\n",
            "\t17 : CC [NUMERICAL]\n",
            "\t16 : EH [NUMERICAL]\n",
            "\t15 : BQ [NUMERICAL]\n",
            "\t14 : GL [NUMERICAL]\n",
            "\t14 : DE [NUMERICAL]\n",
            "\t13 : CR [NUMERICAL]\n",
            "\t11 : CU [NUMERICAL]\n",
            "\t10 : EE [NUMERICAL]\n",
            "\t9 : FL [NUMERICAL]\n",
            "\t9 : EL [NUMERICAL]\n",
            "\t9 : DL [NUMERICAL]\n",
            "\t9 : AX [NUMERICAL]\n",
            "\t8 : FE [NUMERICAL]\n",
            "\t8 : EG [NUMERICAL]\n",
            "\t8 : BC [NUMERICAL]\n",
            "\t7 : EU [NUMERICAL]\n",
            "\t7 : DH [NUMERICAL]\n",
            "\t7 : DA [NUMERICAL]\n",
            "\t7 : CH [NUMERICAL]\n",
            "\t7 : BR [NUMERICAL]\n",
            "\t6 : EB [NUMERICAL]\n",
            "\t6 : DY [NUMERICAL]\n",
            "\t6 : CS [NUMERICAL]\n",
            "\t6 : CD_ [NUMERICAL]\n",
            "\t6 : AH [NUMERICAL]\n",
            "\t5 : FI [NUMERICAL]\n",
            "\t5 : DN [NUMERICAL]\n",
            "\t5 : AF [NUMERICAL]\n",
            "\t4 : GH [NUMERICAL]\n",
            "\t4 : CL [NUMERICAL]\n",
            "\t3 : FD_ [NUMERICAL]\n",
            "\t3 : EP [NUMERICAL]\n",
            "\t3 : DV [NUMERICAL]\n",
            "\t3 : DI [NUMERICAL]\n",
            "\t3 : DF [NUMERICAL]\n",
            "\t3 : AM [NUMERICAL]\n",
            "\t2 : FS [NUMERICAL]\n",
            "\t2 : CB [NUMERICAL]\n",
            "\t2 : BP [NUMERICAL]\n",
            "\t2 : AY [NUMERICAL]\n",
            "\t1 : GI [NUMERICAL]\n",
            "\t1 : GE [NUMERICAL]\n",
            "\t1 : GB [NUMERICAL]\n",
            "\t1 : FC [NUMERICAL]\n",
            "\t1 : CF [NUMERICAL]\n",
            "\t1 : BN [NUMERICAL]\n",
            "\t1 : AR [NUMERICAL]\n",
            "\n",
            "Condition type in nodes:\n",
            "\t374 : HigherCondition\n",
            "Condition type in nodes with depth <= 0:\n",
            "\t130 : HigherCondition\n",
            "Condition type in nodes with depth <= 1:\n",
            "\t374 : HigherCondition\n",
            "Condition type in nodes with depth <= 2:\n",
            "\t374 : HigherCondition\n",
            "Condition type in nodes with depth <= 3:\n",
            "\t374 : HigherCondition\n",
            "Condition type in nodes with depth <= 5:\n",
            "\t374 : HigherCondition\n",
            "\n",
            "Training logs:\n",
            "Number of iteration to final model: 130\n",
            "\tIter:1 train-loss:0.854250 valid-loss:0.833704  train-accuracy:0.823214 valid-accuracy:0.842105\n",
            "\tIter:2 train-loss:0.794809 valid-loss:0.797670  train-accuracy:0.823214 valid-accuracy:0.842105\n",
            "\tIter:3 train-loss:0.749318 valid-loss:0.762543  train-accuracy:0.823214 valid-accuracy:0.842105\n",
            "\tIter:4 train-loss:0.714176 valid-loss:0.743258  train-accuracy:0.823214 valid-accuracy:0.842105\n",
            "\tIter:5 train-loss:0.680427 valid-loss:0.713925  train-accuracy:0.823214 valid-accuracy:0.842105\n",
            "\tIter:6 train-loss:0.649487 valid-loss:0.682638  train-accuracy:0.851786 valid-accuracy:0.859649\n",
            "\tIter:16 train-loss:0.466892 valid-loss:0.596601  train-accuracy:0.933929 valid-accuracy:0.877193\n",
            "\tIter:26 train-loss:0.377456 valid-loss:0.590215  train-accuracy:0.953571 valid-accuracy:0.859649\n",
            "\tIter:36 train-loss:0.315939 valid-loss:0.565237  train-accuracy:0.964286 valid-accuracy:0.859649\n",
            "\tIter:46 train-loss:0.263968 valid-loss:0.557920  train-accuracy:0.969643 valid-accuracy:0.859649\n",
            "\tIter:56 train-loss:0.229165 valid-loss:0.541964  train-accuracy:0.975000 valid-accuracy:0.859649\n",
            "\tIter:66 train-loss:0.202645 valid-loss:0.523548  train-accuracy:0.978571 valid-accuracy:0.859649\n",
            "\tIter:76 train-loss:0.175469 valid-loss:0.528234  train-accuracy:0.982143 valid-accuracy:0.859649\n",
            "\tIter:86 train-loss:0.150488 valid-loss:0.504039  train-accuracy:0.989286 valid-accuracy:0.859649\n",
            "\tIter:96 train-loss:0.130010 valid-loss:0.490322  train-accuracy:0.992857 valid-accuracy:0.859649\n",
            "\tIter:106 train-loss:0.114961 valid-loss:0.487844  train-accuracy:0.994643 valid-accuracy:0.859649\n",
            "\tIter:116 train-loss:0.102846 valid-loss:0.475622  train-accuracy:0.996429 valid-accuracy:0.859649\n",
            "\tIter:126 train-loss:0.089917 valid-loss:0.469268  train-accuracy:0.996429 valid-accuracy:0.877193\n",
            "\tIter:136 train-loss:0.081898 valid-loss:0.464954  train-accuracy:0.996429 valid-accuracy:0.859649\n",
            "\tIter:146 train-loss:0.072915 valid-loss:0.479952  train-accuracy:0.998214 valid-accuracy:0.859649\n",
            "\tIter:156 train-loss:0.064986 valid-loss:0.476820  train-accuracy:0.998214 valid-accuracy:0.859649\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train the model"
      ],
      "metadata": {
        "id": "z8zaWGPTlDU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create list of ids for the creation of oof dataframe.\n",
        "ID_LIST = clean_train.index\n",
        "\n",
        "# Create a dataframe of required size with zero values.\n",
        "oof = pd.DataFrame(data=np.zeros((len(ID_LIST),1)), index=ID_LIST)\n",
        "\n",
        "# Create an empty dictionary to store the models trained for each fold.\n",
        "models = {}\n",
        "\n",
        "# Create empty dict to save metircs for the models trained for each fold.\n",
        "accuracy = {}\n",
        "cross_entropy = {}\n",
        "\n",
        "# Save the name of the label column to a variable.\n",
        "label = \"Class\"\n",
        "\n",
        "# Creates a GroupKFold with 5 splits\n",
        "kf = KFold(n_splits=5)"
      ],
      "metadata": {
        "id": "ctmUQYwZnjA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(dataset_df, class_weight):\n",
        "  # Loop through each fold\n",
        "  for i, (train_index, valid_index) in enumerate(kf.split(X=dataset_df)):\n",
        "    print('##### Fold',i+1)\n",
        "\n",
        "    # Fetch values corresponding to the index\n",
        "    train_df = dataset_df.iloc[train_index]\n",
        "    valid_df = dataset_df.iloc[valid_index]\n",
        "    valid_ids = valid_df.index.values\n",
        "\n",
        "    train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label=label)\n",
        "    valid_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_df, label=label)\n",
        "\n",
        "    # Define the model and metrics\n",
        "    model = tfdf.keras.GradientBoostedTreesModel(max_depth=3, num_trees=700)\n",
        "    model.compile(metrics=[\"accuracy\", \"binary_crossentropy\"])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(x=train_ds, class_weight=class_weight)\n",
        "\n",
        "    # Store the model\n",
        "    models[f\"fold_{i+1}\"] = model\n",
        "\n",
        "    # Predict OOF value for validation data\n",
        "    predict = model.predict(x=valid_ds)\n",
        "\n",
        "    # Store the predictions in oof dataframe\n",
        "    oof.loc[valid_ids, 0] = predict.flatten()\n",
        "\n",
        "    # Evaluate and store the metrics in respective dicts\n",
        "    evaluation = model.evaluate(x=valid_ds,return_dict=True)\n",
        "    accuracy[f\"fold_{i+1}\"] = evaluation[\"accuracy\"]\n",
        "    cross_entropy[f\"fold_{i+1}\"]= evaluation[\"binary_crossentropy\"]\n",
        "\n",
        "  average_loss = 0\n",
        "  average_acc = 0\n",
        "\n",
        "  for _model in  models:\n",
        "      average_loss += cross_entropy[_model]\n",
        "      average_acc += accuracy[_model]\n",
        "      print(f\"\\n{_model}: acc: {accuracy[_model]:.4f} loss: {cross_entropy[_model]:.4f}\")\n",
        "\n",
        "  print(f\"\\nAverage accuracy: {average_acc/5:.4f}  Average loss: {average_loss/5:.4f}\")\n",
        "\n",
        "  return model\n",
        "\n",
        "model = train_model(clean_train, class_weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0gqqCd0eck5",
        "outputId": "f86eea6f-d32e-4c38-88dd-5527d2b508ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##### Fold 1\n",
            "Warning: Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use /tmp/tmp4gp4zvpb as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:01.510059. Found 493 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:00.742427\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "1/1 [==============================] - 0s 175ms/step\n",
            "1/1 [==============================] - 1s 762ms/step - loss: 0.0000e+00 - accuracy: 0.9597 - binary_crossentropy: 0.1458\n",
            "##### Fold 2\n",
            "Warning: Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use /tmp/tmpyzoumg97 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:01.226315. Found 493 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:00.444301\n",
            "Compiling model...\n",
            "Model compiled.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function InferenceCoreModel.make_predict_function.<locals>.predict_function_trained at 0x7d620c107370> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.0000e+00 - accuracy: 0.8790 - binary_crossentropy: 0.2557\n",
            "##### Fold 3\n",
            "Warning: Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use /tmp/tmp33omkhbd as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:00.753258. Found 494 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:00.630651\n",
            "Compiling model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function InferenceCoreModel.make_predict_function.<locals>.predict_function_trained at 0x7d61c6189b40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model compiled.\n",
            "1/1 [==============================] - 0s 349ms/step\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.0000e+00 - accuracy: 0.9268 - binary_crossentropy: 0.2221\n",
            "##### Fold 4\n",
            "Warning: Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use /tmp/tmpvwb6l4p7 as temporary training directory\n",
            "Reading training dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function CoreModel._consumes_training_examples_until_eof at 0x7d623420e7a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset read in 0:00:00.779396. Found 494 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:00.242387\n",
            "Compiling model...\n",
            "Model compiled.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function InferenceCoreModel.yggdrasil_model_path_tensor at 0x7d62200e3d00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.0000e+00 - accuracy: 0.9187 - binary_crossentropy: 0.2841\n",
            "##### Fold 5\n",
            "Warning: Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use /tmp/tmp6pzkc80r as temporary training directory\n",
            "Reading training dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function CoreModel._consumes_training_examples_until_eof at 0x7d623420e7a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset read in 0:00:00.759190. Found 494 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:00.534991\n",
            "Compiling model...\n",
            "Model compiled.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function InferenceCoreModel.yggdrasil_model_path_tensor at 0x7d61ddb59fc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 126ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function InferenceCoreModel.make_test_function.<locals>.test_function at 0x7d61c6189900> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 297ms/step - loss: 0.0000e+00 - accuracy: 0.9512 - binary_crossentropy: 0.1555\n",
            "\n",
            "fold_1: acc: 0.9597 loss: 0.1458\n",
            "\n",
            "fold_2: acc: 0.8790 loss: 0.2557\n",
            "\n",
            "fold_3: acc: 0.9268 loss: 0.2221\n",
            "\n",
            "fold_4: acc: 0.9187 loss: 0.2841\n",
            "\n",
            "fold_5: acc: 0.9512 loss: 0.1555\n",
            "\n",
            "Average accuracy: 0.9271  Average loss: 0.2126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Save the Model\n"
      ],
      "metadata": {
        "id": "r9RuWAtX70CI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/My Drive/ICR_Project/ICR_model.keras', save_format=\"keras\")"
      ],
      "metadata": {
        "id": "q4Zn0D6M7yh-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}